{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "n_actions = env.action_space.shape[0]\n",
    "n_states = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise:\n",
    "    def __init__(self, mu, theta, sigma, action_dim):\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.action_dim = action_dim\n",
    "        self.state = self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        return np.full(self.action_dim, self.mu)\n",
    "\n",
    "    def make_noise(self):\n",
    "        delta = self.theta *(self.mu - self.state) + self.sigma * np.random.randn(len(self.state))\n",
    "        self.state = self.state + delta\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size_1, hidden_size_2, input_size, action_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1 + action_size, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size_1, hidden_size_2, input_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Метод Q_estimate - считает Q(s, a) при помощи онлайн Критик сети\n",
    "* Метод Q_target - тоже самое, при помощи таргет Критик сети\n",
    "* Методы action_estimate и action_target - аналогично, только для вычисления действий от стейта при помощи Актор сетей\n",
    "* Методы update_*_params делают бэкпроп и градиентные шаги для соответствующих сетей\n",
    "* Метод Actor_loss - лосс функция для Актор сети\n",
    "* Метод update_target_networks - обновляет таргет сети при помощи софт апдейт метода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, layers_sizes, polyak=0.9999,\\\n",
    "                 critic_lr=0.0001, critic_optim_method=optim.Adam, critic_loss=F.mse_loss, \\\n",
    "                 actor_lr=0.0001, actor_optim_method=optim.Adam):\n",
    "        self.polyak = polyak\n",
    "\n",
    "        self.CriticEstimate = ValueNetwork(*layers)\n",
    "        self.CriticTarget = ValueNetwork(*layers)\n",
    "        self.CriticTarget.load_state_dict(self.CriticEstimate.state_dict())\n",
    "        self.critic_loss = critic_loss\n",
    "        self.critic_optimizer = critic_optim_method(self.CriticEstimate.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.ActorEstimate = PolicyNetwork(*layers)\n",
    "        self.ActorTarget = PolicyNetwork(*layers)\n",
    "        self.ActorTarget.load_state_dict(self.ActorEstimate.state_dict())\n",
    "        self.actor_optimizer = actor_optim_method(self.ActorEstimate.parameters(), lr=actor_lr)\n",
    "\n",
    "    def Q_estimate(self, state, action):\n",
    "        return self.CriticEstimate(state, action)\n",
    "\n",
    "    def Q_target(self, state, action):\n",
    "        return self.CriticTarget(state, action)\n",
    "\n",
    "    def action_estimate(self, state):\n",
    "        return self.ActorEstimate(state)\n",
    "\n",
    "    def action_target(self, state):\n",
    "        return self.ActorTarget(state)\n",
    "\n",
    "    def update_critic_params(self, estimates, targets):\n",
    "        loss = self.critic_loss(estimates, targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for param in self.CriticEstimate.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def update_actor_params(self, states):\n",
    "        loss = self.actor_loss(states)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        for param in self.ActorEstimate.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def actor_loss(self, states):\n",
    "        actions = self.action_estimate(states)\n",
    "        return -self.Q_estimate(states, actions).mean()\n",
    "\n",
    "    def update_target_networks(self):\n",
    "        self.soft_update(self.ActorEstimate, self.ActorTarget)\n",
    "        self.soft_update(self.CriticEstimate, self.CriticTarget)\n",
    "\n",
    "    def soft_update(self, estimate_model, target_model):\n",
    "        for estimate_param, target_param in zip(estimate_model.parameters(), target_model.parameters()):\n",
    "            target_param.data.copy_(target_param.data * self.polyak + estimate_param * (1 - self.polyak))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.ActorEstimate, name)\n",
    "        print('------ Model saved ------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Использует ДДПГ для обучения\n",
    "* Первые **exploration episodes** делает действия, которые сэмплятся из равномерного распределения\n",
    "* В последующих эпизодах сэмплит действия при помощи Актор сети + нойз\n",
    "* Использует experience replay после каждого шага\n",
    "* Софт апдейт таргет сетей в конце каждого шага\n",
    "* Собираем скор каждого эпизода в список. Если пять раз подряд средний скор за последние 10 эпизодов был больше 91, то останавливаем обучение\n",
    "* Использует модифицированную награду - плюс 10 * модуль скорости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Метод act - делает действие с помощью Актор сети\n",
    "* Метод optimize - это фактически experience replay\n",
    "* Метод train - обучение агента\n",
    "* Метод test - тестирует агента на заданном количестве эпизодов\n",
    "* Метод save - сохраняет Актор сеть в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, Model, noise, goal, min_score, \\\n",
    "                gamma=0.99, batch_size=64, memory_size=100000, max_episode=2000, upd_rate=1, exploration_episodes=10):\n",
    "        self.env = env\n",
    "        self.Model = Model\n",
    "        self.noise = noise\n",
    "        self.goal = goal\n",
    "        self.min_score = min_score\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        self.max_episode = max_episode\n",
    "        self.target_update_rate = upd_rate\n",
    "        self.exploration_episodes = exploration_episodes\n",
    "\n",
    "    def act(self, state, eps):\n",
    "        with torch.no_grad():\n",
    "            action = self.Model.action_estimate(state)\n",
    "            noise = torch.tensor(eps * self.noise.make_noise()).unsqueeze(0)\n",
    "            action += noise\n",
    "        return action.clamp_(self.env.action_space.low[0], self.env.action_space.high[0])\n",
    "\n",
    "    def optimize(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_batch = torch.cat(batch.done)\n",
    "\n",
    "        estimates = self.Model.Q_estimate(state_batch, action_batch)\n",
    "        Q_next = torch.zeros(self.batch_size, device=device).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.Model.action_target(next_state_batch)\n",
    "            Q_next[~done_batch] = self.Model.Q_target(next_state_batch, next_actions)[~done_batch]\n",
    "        targets = reward_batch.unsqueeze(1) + self.gamma * Q_next\n",
    "        self.Model.update_critic_params(estimates, targets)\n",
    "        self.Model.update_actor_params(state_batch)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        all_scores = []\n",
    "        successful_sequences = 0\n",
    "        step = 0\n",
    "        eps = 1\n",
    "        for ep in range(1, self.max_episode + 1):\n",
    "            state = self.env.reset()\n",
    "            state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                if ep > self.exploration_episodes:\n",
    "                    action = self.act(state, eps)\n",
    "                else:\n",
    "                    action = torch.tensor([np.random.uniform(self.env.action_space.low[0], self.env.action_space.high[0])])\n",
    "                    action = action.unsqueeze(0)\n",
    "                action = torch.tensor(action).to(device)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "                modified_reward = reward + 10 * abs(next_state[1])\n",
    "\n",
    "                next_state = torch.tensor(next_state).to(device).float().unsqueeze(0)\n",
    "                modified_reward = torch.tensor(modified_reward).to(device).float().unsqueeze(0)\n",
    "                done = torch.tensor(done).to(device).unsqueeze(0)\n",
    "\n",
    "                self.memory.push(state, action, next_state, modified_reward, done)\n",
    "                state = next_state\n",
    "\n",
    "                self.optimize()\n",
    "\n",
    "                step += 1\n",
    "                if step % self.target_update_rate == 0:\n",
    "                    self.Model.update_target_networks()\n",
    "\n",
    "                if done:\n",
    "                    if episode_reward > self.min_score:\n",
    "                        print(episode_reward, 'at episdoe', ep)\n",
    "\n",
    "            eps = max(eps * 0.95, 0.1)\n",
    "            all_scores.append(episode_reward)\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                print('episode', ep, ':', np.mean(all_scores[:-10:-1]), 'average score')\n",
    "\n",
    "            if np.mean(all_scores[:-15:-1]) >= self.goal:\n",
    "                successful_sequences += 1\n",
    "                if successful_sequences == 5:\n",
    "                    print('success at episode', ep)\n",
    "                    return all_scores\n",
    "            else:\n",
    "                successful_sequences = 0\n",
    "\n",
    "        return all_scores\n",
    "\n",
    "    def test(self, episodes=100, render=False):\n",
    "        state = self.env.reset()\n",
    "        state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "        ep_count = 0\n",
    "        current_episode_reward = 0\n",
    "        scores = []\n",
    "        while ep_count < episodes:\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            action = self.act(state, 0)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "            current_episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                ep_count += 1\n",
    "                scores.append(current_episode_reward)\n",
    "                current_episode_reward = 0\n",
    "                state = self.env.reset()\n",
    "                state = torch.tensor(state).to(device).float().unsqueeze(0)\n",
    "\n",
    "        print('average score:', sum(scores) / len(scores))\n",
    "        print('max reward:', max(scores))\n",
    "        print('-----')\n",
    "        print()\n",
    "\n",
    "    def save(self, name='DDPG_agent.pkl'):\n",
    "        self.Model.save(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовим агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = (256, 256, n_states, n_actions)\n",
    "\n",
    "Model = DDPG(layers, polyak=0.999, \\\n",
    "            critic_lr=0.001, critic_optim_method=optim.Adam, critic_loss=F.mse_loss, \\\n",
    "            actor_lr=0.0001, actor_optim_method=optim.Adam)\n",
    "\n",
    "noise = Noise(0, 0.15, 0.2, n_actions)\n",
    "\n",
    "MountainCarAgent = Agent(env, Model, noise, goal=91, min_score=-100, \\\n",
    "                gamma=0.9, batch_size=128, memory_size=20000, max_episode=100, upd_rate=1, exploration_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запустим обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.474966296827183 at episdoe 1\n",
      "-32.3332240106789 at episdoe 2\n",
      "-33.21047735648885 at episdoe 3\n",
      "-32.50320925156587 at episdoe 4\n",
      "-33.31074066166956 at episdoe 5\n",
      "-33.117876987029035 at episdoe 6\n",
      "-33.60144792398803 at episdoe 7\n",
      "-34.48103695576966 at episdoe 8\n",
      "-32.990729177490145 at episdoe 9\n",
      "-34.96613735812459 at episdoe 10\n",
      "episode 10 : -33.3905421869783 average score\n",
      "96.90255742849581 at episdoe 11\n",
      "95.40419095742763 at episdoe 12\n",
      "95.64473290673475 at episdoe 13\n",
      "95.21930040929921 at episdoe 14\n",
      "94.77367090795892 at episdoe 15\n",
      "94.60065728532072 at episdoe 16\n",
      "96.33985691272693 at episdoe 17\n",
      "95.86782104828362 at episdoe 18\n",
      "96.34561538256686 at episdoe 19\n",
      "95.41110178715576 at episdoe 20\n",
      "episode 20 : 95.51188306638603 average score\n",
      "96.19968165833617 at episdoe 21\n",
      "94.5589985649333 at episdoe 22\n",
      "95.73672180062093 at episdoe 23\n",
      "92.7515445953659 at episdoe 24\n",
      "91.5378381311591 at episdoe 25\n",
      "94.22952361989223 at episdoe 26\n",
      "90.53419155856346 at episdoe 27\n",
      "91.33060599457602 at episdoe 28\n",
      "success at episode 28\n"
     ]
    }
   ],
   "source": [
    "scores = MountainCarAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Протестируем агента на 100 эпизодах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score: 92.04654421247142\n",
      "max reward: 94.95437940711322\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MountainCarAgent.test(episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построим график обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить, первые 10 эпизодов агент делает абсолютно рандомные действия из равномерного распределения.\n",
    "В среднем это дает -33 скора за каждый эпизод, все это время агент так же сохраняет в буфер транзишны и обучается на батчах. Таким образом, по идее, к 11 эпизоду агент гарантированно выбивает 90+- скора и в локальный минимум не попадает. В итоге за последующие ~20 эпизодов агент обучается на действиях, которые дают нормальный скор. После этого обучение автоматом останавливается и обученный агент способен решить поставленную задачу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes = range(len(scores))\n",
    "plt.plot(episodes, scores)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('target net average score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
